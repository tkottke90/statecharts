<?xml version="1.0" encoding="UTF-8"?>
<scxml xmlns="http://www.w3.org/2005/07/scxml"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="../schemas/scxml.xsd"
       version="1.0"
       datamodel="ecmascript"
       initial="agents">

  <datamodel>
    <data id="ollamaUrl" expr="'http://localhost:11434'"/>
    <data id="callLLM.model" expr="'mistral:7b'"/>
    <data id="message">I would like an LLM to summarize a youtube transcript for me</data>
    <data id="evalCount" expr="0"/>
    <data id="error" expr="null"/>
    <data id="messages" expr="[]"/>
  </datamodel>

  <state id="callLLM" initial="httpRequest">
    <datamodel>
      <!-- Generate unique send ID for tracking this request -->
      <data id="sendId" expr="'ollama_' + Date.now()" />
    </datamodel>

    <state id="httpRequest">
      <onentry>

        <log expr="'Sending HTTP request with send ID: ' + data.sendId" />

        <!-- Make HTTP POST request to Ollama API -->
        <send 
          type="http"
          event="ollama"
          targetexpr="data.ollamaUrl + '/api/chat'"
          method="POST"
        >
          <param name="model" expr="data.callLLM.model" />
          <param name="json" expr="data.model_json ?? false" />
          <param name="messages" expr="data.messages" />
          <param name="options.temperature" expr="data.model_temp ?? 0.5" />
          <param name="options.top_k" expr="data.model_top_k ?? undefined" />
          <param name="options.top_p" expr="data.model_top_p ?? undefined" />
          <param name="options.seed" expr="data.model_seed ?? undefined" />
          <param name="stream" expr="false" />
        </send>

        <log expr="`HTTP Request Complete - ${data.sendId}`"></log>
      </onentry>
    </state>

    <state id="onSuccess">
      <onentry>
        <log expr="'Event SendId ' + JSON.stringify(_event)" />
        <!-- <log expr="'Received HTTP response: ' + JSON.stringify(_event.data, null, 2)" /> -->
      
        <if cond="_event.sendid == data.sendId">
          <assign location="data.llmResponse.isJSON" expr="false" />
          <assign location="data.messages" expr="data.messages.concat({ role: _event.data.body.message.role, content: _event.data.body.message.content })" />
          <assign location="data.evalCount" expr="data.evalCount + _event.data.body.eval_count" />
        
          <else>
            <log>Send ID mismatch</log>
          </else>
        </if>

        <raise event="onSuccess.done" />
      </onentry>

      <transition event="onSuccess.done" target="done" />
    </state>

    <onexit>
      <!-- Cleanup variables created by state -->
      <assign location="data.callLLM" clear="true" />
      <assign location="data.sendId" clear="true" />
    </onexit>

    <!-- Handle successful response & Append Message and Eval Count -->
    <transition event="http.response" target="callLLM.onSuccess" />

    <!-- Handle HTTP errors -->
    <transition event="http.error" target="handleError">
      <log expr="'Received HTTP error: ' + JSON.stringify(_event.data)" />
      <assign location="data.error" expr="_event.data" />
    </transition>

    <!-- Handle timeout or other failures -->
    <transition event="error.communication" target="handleError">
      <log expr="'Received communication error: ' + JSON.stringify(_event.data)" />
      <assign location="data.error" expr="_event.data" />
    </transition>

    <transition event="error.*" target="error" />
  </state>

  <state id="handleError">
    <onentry>
      <!-- Log the error -->
      <log expr="'Ollama API error: ' + JSON.stringify(data.error)" />

      <!-- Set a default error message -->
      <assign location="data.message" expr="'Error: Failed to get response from Ollama'" />

      <!-- Transition back to idle -->
      <raise event="error.handled" />
    </onentry>

    <transition event="error.handled" target="done" />
  </state>

  <state id="agents" initial="promptAuthor">
    <state id="promptAuthor">
      <datamodel>
        <data id="systemPrompt">
         ## Role

         You are an AI Prompt Author.  Your task is to refine a users idea into an LLM Prompt

         ## Guidelines
         
         - Analyze the following prompt idea: [insert prompt idea]
         - Rewrite the prompt for clarity and effectiveness
         - Identify potential improvements or additions
         - Refine the prompt based on identified improvements
         - Present the final optimized prompt
        </data>
      </datamodel>
    
      <onentry>
        <if cond="data.message?.length === 0">
          <log>No message provided</log>
          <raise event="error.no_message" />
        </if>

        <if cond="data.messages.length === 0">
          <log>Adding System Message</log>
          <assign location="data.messages" expr="data.messages.concat([ { role: 'system', content: data.systemPrompt } ])" />
          <assign location="data.messages" expr="data.messages.concat( { role: 'user', content: data.message } )" />
        </if>

        <raise event="prompt.ready" />
      </onentry>

      <onexit>
        <assign location="data.systemPrompt" clear="true"></assign>
      </onexit>

      <!-- Add transition that responds to the raised event -->
      <transition event="prompt.ready" target="callLLM"/>
    </state>
  </state>

  <final id="done" />

  <final id="error">
    <log expr="'Error: ' + _event.name" />
  </final>
</scxml>